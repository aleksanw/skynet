"""
Based on the code from Open AI

The MIT License

Copyright (c) 2017 OpenAI (http://openai.com)

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in
all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.

"""

import numpy as np
import random

import logging
logger = logging.getLogger()#"pdqfd")

from segment_tree import SumSegmentTree, MinSegmentTree
import tables

from q_network import MASK_VALUE


class ReplayBuffer(object):
    def __init__(self, size, state_shape, n_batch_trajectories, n_trajectory_steps, n_emus=1):
        """Create Prioritized Replay buffer.

        Parameters
        ----------
        size: int
            Max number of self-generated transitions to store in the buffer. 
            When the buffer overflows, the old memories are dropped
        n_emus: int
            Number of emulators. Each emulator will store samples in its own buffer, 
            but sampling will happen accross buffers 
        """

        # The buffer is logically divided in n_emu segments,
        # where each of the segments (of equal size) are reserved for data
        # generated by each of the emulators

        # We need to ensure that each emulator segment has the same size
        self._emu_buffer_size = 0 if n_emus == 0 else round(size / n_emus)
        self._size = self._emu_buffer_size * n_emus

        self._n_emu = n_emus

        self.n_trajectory_steps = n_trajectory_steps
        self.n_batch_trajectories = n_batch_trajectories

        # The actual buffers where the data is stored.
        self.obses_t = np.empty((self._size,) + state_shape)
        self.obses_tp1 = np.empty((self._size,) + state_shape)
        self.actions = np.empty((self._size), dtype=np.int16)
        self.rewards = np.empty((self._size))
        self.dones = np.empty((self._size), dtype=np.bool)
        self.traj_ids = np.ones((self._size), dtype=np.int8) * -1

        # Arrays to hold sampled data
        self.batch_obses_t = np.empty((n_batch_trajectories, n_trajectory_steps,) + state_shape)
        self.batch_obses_tp1 = np.empty((n_batch_trajectories, n_trajectory_steps,) + state_shape)
        self.batch_actions = np.empty((n_batch_trajectories, n_trajectory_steps), dtype=np.int16)
        self.batch_rewards = np.empty((n_batch_trajectories, n_trajectory_steps))
        self.batch_dones = np.empty((n_batch_trajectories, n_trajectory_steps), dtype=np.bool)
        self.mask_s = np.ones((n_trajectory_steps,)+ state_shape) * MASK_VALUE
        self.mask_other = np.ones(n_trajectory_steps) * MASK_VALUE

        # Number of trasitions in each emulator sub-buffer at each point in time. Needed for sampling.
        # When beginning to sample, it is assume that each emulator sub-buffer has the same number of samples
        self._n_samples = 0

        self._next_idx_emu = [i*self._emu_buffer_size for i in range(n_emus)]

        # Since the emulator buffers are cyclic, and in order to be able to say whether two consecutive samples belong
        # to the same trajectory or not, we need to mark all samples with a traj_id. Fo this we make _current_traj_id
        # alternate between 0 and 1 for "even" passes through the buffer, and between 2 and 3 for "odd" passes.
        # See current_traj_id()
        # Obs! This is not needed for demo data, since the demo buffer is not cyclic and all demo trajectories end with
        # a 'done'. We can thus use the 'dones' to split trajectories
        self._current_traj_id = [0] * n_emus
        self._traj_id_flag = [0] * n_emus


    def __len__(self):
        return self._n_samples

    def current_traj_id(self, emu_id, done):
        id = self._current_traj_id[emu_id]
        if self._traj_id_flag[emu_id] == 0 and done: # Alternate between 0 and 1 for the trajectory ids
            self._current_traj_id[emu_id] = (self._current_traj_id[emu_id] + 1) % 2
        elif self._traj_id_flag[emu_id] == 1 and done: # Alternate between 2 and 3 for the trajectory ids
            self._current_traj_id[emu_id] = (self._current_traj_id[emu_id] - 1) % 2 + 2

        if self._next_idx_emu[emu_id] % self._emu_buffer_size == self._emu_buffer_size - 1:
            # We reached the end of the emulator's buffer. Next sample will be inserted at the beginning of the buffer, so
            # change the set of trajectory ids to be used next
            self._traj_id_flag[emu_id] = (self._traj_id_flag[emu_id] + 1) % 2

        return id

    # emu_id: 0-based id of the emulator adding the sample
    # OBS! It is expected that all demo data has been added before any self-generated data is added
    def add(self, obs_t, action, reward, obs_tp1, done, emu_id):
        idx = self._next_idx_emu[emu_id]

        self.obses_t[idx] = obs_t
        self.obses_tp1[idx] = obs_tp1
        self.actions[idx] = action
        self.rewards[idx] = reward
        self.dones[idx] = done
        #Obs! current_traj_id() must be invoked before updating self._next_idx_emu[emu_id]
        self.traj_ids[idx] = self.current_traj_id(emu_id, done)

        # cyclic storage inside each emulator's sub-buffer
        self._next_idx_emu[emu_id] = emu_id*self._emu_buffer_size + (
            self._next_idx_emu[emu_id] + 1) % self._emu_buffer_size

        if emu_id == 0:
            self._n_samples = min(self._emu_buffer_size, self._n_samples + 1)

        return idx


    def _retrieve_samples(self, idxes):
        obses_t, actions, rewards, obses_tp1, dones = [], [], [], [], []

        for i in idxes:
            obses_t.append(self.obses_t[i])
            actions.append(self.actions[i])
            rewards.append(self.rewards[i])
            obses_tp1.append(self.obses_tp1[i])
            dones.append(self.dones[i])

        return (np.array(obses_t), np.array(actions), np.array(rewards), np.array(obses_tp1), np.array(dones))

    def _retrieve_n_step_trajectories(self, traj_idxes):
        # Hack: shift traj_idxes a bit
        traj_idxes += 5
        idxes = []
        for k, idx in enumerate(traj_idxes):
            # Randomly choose n_steps samples around current idx
            emu_id = idx // self._emu_buffer_size
            current_traj_id = self.traj_ids[idx]

            max_idx = min(emu_id * self._emu_buffer_size + self._n_samples, idx + self.n_trajectory_steps)
            idx_new_traj_after = np.where(self.traj_ids[idx + 1:max_idx] != current_traj_id)[0]
            if idx_new_traj_after.size > 0:
                max_idx = idx + idx_new_traj_after[0] + 1

            min_idx = max(emu_id * self._emu_buffer_size, idx - self.n_trajectory_steps + 1)
            idx_new_traj_before = np.where(self.traj_ids[min_idx:idx] != current_traj_id)[0]
            if idx_new_traj_before.size > 0:
                min_idx = min_idx + idx_new_traj_before[-1] + 1

            max_samples = max_idx - min_idx  # Max number of available samples for this trajectory
            if max_samples > self.n_trajectory_steps:  # Return n_trajectory_steps random consecutive samples
                min_idx = np.random.randint(min_idx, max_idx - self.n_trajectory_steps + 1)
                max_idx = min_idx + self.n_trajectory_steps
                n_masks = 0
            else:  # Return all available samples and mask the empty slots
                n_masks = self.n_trajectory_steps - max_samples
                self.batch_obses_t[k, 0:n_masks] = self.mask_s[0:n_masks]
                self.batch_obses_tp1[k, 0:n_masks] = self.mask_s[0:n_masks]
                self.batch_actions[k, 0:n_masks] = self.mask_other[0:n_masks]
                self.batch_rewards[k, 0:n_masks] = self.mask_other[0:n_masks]
                self.batch_dones[k, 0:n_masks] = self.mask_other[0:n_masks]
                # Add -1 as idx for masked samples.
                idxes.extend([-1 for _ in range(n_masks)])

            self.batch_obses_t[k, n_masks:] = self.obses_t[min_idx:max_idx]
            self.batch_obses_tp1[k, n_masks:] = self.obses_tp1[min_idx:max_idx]
            self.batch_actions[k, n_masks:] = self.actions[min_idx:max_idx]
            self.batch_rewards[k, n_masks:] = self.rewards[min_idx:max_idx]
            self.batch_dones[k, n_masks:] = self.dones[min_idx:max_idx]
            idxes.extend([i for i in range(min_idx, max_idx)])

            #for i in range(min_idx, max_idx):
            #    assert self.traj_ids[i] == self.traj_ids[min_idx]

        return (self.batch_obses_t, self.batch_actions, self.batch_rewards, self.batch_obses_tp1, self.batch_dones), np.array(idxes)

    def _get_indexes(self, n):
        # Sample the emu sub-buffers.
        buff_id = np.array([random.randint(0, self._n_emu - 1) for _ in range(n)])
        # Sample from the emulator sub-buffers
        # Obs! self._n_samples is the number of transitions on any emulator sub-buffer
        emu_idxes = np.array([random.randint(0, self._n_samples - 1) for _ in range(n)])
        idxes = buff_id * self._emu_buffer_size + emu_idxes

        return idxes

    def sample(self, batch_size):
        """Sample a batch of experiences.
        For correct behavior, the same number of transitions for each emulator should be stored in the buffer 
        when sampling 

        Parameters
        ----------
        batch_size: int
            How many transitions to sample.

        Returns
        -------
        obs_batch: np.array
            batch of observations
        act_batch: np.array
            batch of actions executed given obs_batch
        rew_batch: np.array
            rewards received as results of executing act_batch
        next_obs_batch: np.array
            next set of observations seen after executing act_batch
        done_mask: np.array
            done_mask[i] = 1 if executing act_batch[i] resulted in
            the end of an episode and 0 otherwise.
        """
        idxes = self._get_indexes(batch_size)
        exp =  self._retrieve_samples(idxes)
        return exp + (np.ones_like(idxes), idxes)

    def sample_nstep(self):
        """Sample a (self.n_batch_trajectories, self.n_trajectory_steps, n_s) batch of states, where n_s is the dimension of the 
         state vector


        Returns
        -------
        obs_batch: np.array
            batch of observations
        act_batch: np.array
            batch of actions executed given obs_batch
        rew_batch: np.array
            rewards received as results of executing act_batch
        next_obs_batch: np.array
            next set of observations seen after executing act_batch
        done_mask: np.array
            done_mask[i] = 1 if executing act_batch[i] resulted in
            the end of an episode and 0 otherwise.
        n_step_rewards_batch: np.array
            n-step rewards vector batch
        tpn_obs_batch: np.array
            tpn set of observations
        n_tpn_step_batch: np.array
            n in n-step indicator to indicate if trajectory sampled 
            is unfinished or done -- trajectory is unfinished if 
            there are no more transitions to cover all n steps
        n_step_done_mask: np.array
            n_step_done_mask[i] = 1 if trajectory sampled reaches 
            the end of an episode, and 0 otherwise.
        """
        traj_idxes = self._get_indexes(self.n_batch_trajectories)
        trajectories, idxes = self._retrieve_n_step_trajectories(traj_idxes, self.n_trajectory_steps)
        return trajectories + (np.ones_like(idxes), idxes)

class PrioritizedReplayBuffer(ReplayBuffer):
    def __init__(self, size, state_shape, alpha, n_batch_trajectories, n_trajectory_steps, n_emus=1):
        """Create Prioritized Replay buffer.

        Parameters
        ----------
        size: int
            Max number of transitions to store in the buffer. When the buffer
            overflows the old memories are dropped.
        dsize: int
            Max number of demonstration transitions. These are retained in the 
            buffer permanently.
            https://arxiv.org/abs/1704.03732
        alpha: float
            how much prioritization is used
            (0 - no prioritization, 1 - full prioritization)

        See Also
        --------
        ReplayBuffer.__init__
        """
        super(PrioritizedReplayBuffer, self).__init__(size, state_shape, n_batch_trajectories, n_trajectory_steps, n_emus=n_emus)
        assert alpha > 0
        self._alpha = alpha

        it_capacity = 1
        while it_capacity < self._size:
            it_capacity *= 2

        self._it_sum = SumSegmentTree(it_capacity)
        self._it_min = MinSegmentTree(it_capacity)
        self._max_priority = 1.0

    def add(self, *args, **kwargs):
        """See ReplayBuffer.add_effect"""
        idx = super().add(*args, **kwargs)
        self._it_sum[idx] = self._max_priority ** self._alpha
        self._it_min[idx] = self._max_priority ** self._alpha


    def _sample_proportional(self, batch_size):
        res = []
        for _ in range(batch_size):
            # TODO(szymon): should we ensure no repeats?
            mass = random.random() * self._it_sum.sum(0, self._size -1)
            idx = self._it_sum.find_prefixsum_idx(mass)
            res.append(idx)
        return np.array(res)

    def _compute_weights(self, idxes, beta):
        weights = []
        p_min = self._it_min.min() / self._it_sum.sum()
        max_weight = (p_min * self._size) ** (-beta)

        for idx in idxes:
            if idx < 0:
                weights.append(0.0)
                continue
            p_sample = self._it_sum[idx] / self._it_sum.sum()
            weight = (p_sample * self._size) ** (-beta)
            weights.append(weight / max_weight)

        return np.array(weights)

    def sample(self, batch_size, beta):
        """Sample a batch of experiences.

        compared to ReplayBuffer.sample
        it also returns importance weights and idxes
        of sampled experiences.


        Parameters
        ----------
        batch_size: int
            How many transitions to sample.
        beta: float
            To what degree to use importance weights
            (0 - no corrections, 1 - full correction)

        Returns
        -------
        obs_batch: np.array
            batch of observations
        act_batch: np.array
            batch of actions executed given obs_batch
        rew_batch: np.array
            rewards received as results of executing act_batch
        next_obs_batch: np.array
            next set of observations seen after executing act_batch
        done_mask: np.array
            done_mask[i] = 1 if executing act_batch[i] resulted in
            the end of an episode and 0 otherwise.
        weights: np.array
            Array of shape (batch_size,) and dtype np.float32
            denoting importance weight of each sampled transition
        idxes: np.array
            Array of shape (batch_size,) and dtype np.int32
            idexes in buffer of sampled experiences
        """
        assert beta > 0

        idxes = self._sample_proportional(batch_size)
        batch_samples = self._retrieve_samples(idxes)
        weights = self._compute_weights(idxes, beta)
        return tuple(list(batch_samples) + [weights, idxes])

    def sample_nstep(self, beta):
        """Sample a (self.n_batch_trajectories, self.n_trajectory_steps, n_s) batch of states, where n_s is the dimension of the 
         state vector

        Compared to ReplayBuffer.sample
        it also returns importance weights and idxes
        of sampled experiences.


        Parameters
        ----------
        batch_size: int
            How many transitions to sample.
        beta: float
            To what degree to use importance weights
            (0 - no corrections, 1 - full correction)
        n_step: int
            How many steps to look into the future

        Returns
        -------
        obs_batch: np.array
            batch of observations
        act_batch: np.array
            batch of actions executed given obs_batch
        rew_batch: np.array
            rewards received as results of executing act_batch
        next_obs_batch: np.array
            next set of observations seen after executing act_batch
        done_mask: np.array
            done_mask[i] = 1 if executing act_batch[i] resulted in
            the end of an episode and 0 otherwise.
        n_step_rewards_batch: np.array
            n-step rewards vector batch
        tpn_obs_batch: np.array
            tpn set of observations
        n_tpn_step_batch: np.array
            n in n-step indicator to indicate if trajectory sampled 
            is unfinished or done -- trajectory is unfinished if 
            there are no more transitions to cover all n steps
        n_step_done_mask: np.array
            n_step_done_mask[i] = 1 if trajectory sampled reaches 
            the end of an episode, and 0 otherwise.
        weights: np.array
            Array of shape (batch_size,) and dtype np.float32
            denoting importance weight of each sampled transition
        idxes: np.array
            Array of shape (batch_size,) and dtype np.int32
            idexes in buffer of sampled experiences
        """
        assert beta > 0

        traj_idxes = self._sample_proportional(self.n_batch_trajectories)
        batched_trajectories, idxes = self._retrieve_n_step_trajectories(traj_idxes)

        #for i in idxes:
        #    if i == -1: continue
        #    assert self.traj_ids[i] > -1

        weights = self._compute_weights(idxes, beta)
        return batched_trajectories + (weights, idxes)

    def update_priorities(self, idxes, priorities):
        """Update priorities of sampled transitions.

        sets priority of transition at index idxes[i] in buffer
        to priorities[i].

        Parameters
        ----------
        idxes: [int]
            List of idxes of sampled transitions
        priorities: [float]
            List of updated priorities corresponding to
            transitions at the sampled idxes denoted by
            variable `idxes`.
        """
        assert len(idxes) == len(priorities)
        for idx, priority in zip(idxes, priorities):
            if idx < 0: 
                continue
            assert priority > 0
            assert 0 <= idx < self._size
            self._it_sum[idx] = priority ** self._alpha
            self._it_min[idx] = priority ** self._alpha
            self._max_priority = max(self._max_priority, priority)

if __name__ == '__main__':
    n_steps = 6
    n_emus = 5
    replay_buffer = PrioritizedReplayBuffer(100, (1,), 0.6, 9, n_steps, n_emus=n_emus)
    s = np.ones((1))
    for _ in range(10000):
        for e in range(n_emus):
            for i in range(n_steps):
                replay_buffer.add(s*i, i, i, s*i, np.random.randint(2), e)
        experience = replay_buffer.sample_nstep(beta=0.1)
    print("DONE")
    #print(experience)
